# Linear Regression Least Squares

In this jupyter notebook I explore the math behind one of the simplest **Linear Regression** method.

Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y).

The most common technique is called **Ordinary Least Squares**:
*given a regression line through the data we calculate the distance from each data point to the regression line, square it, and sum all of the squared errors together.*

![linear_least_squares](https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg)

When there are a larger number of features or too many training instances to fit in memory, the use of different ways such as **Gradient Descent** is recommended.

### References
- [Wikipedia](https://en.wikipedia.org/wiki/Linear_least_squares)
