<h1>Linear Regression Least Squares</h1>

In this jupyter notebook I explore the math behind one of the simplest <b>Linear Regression</b> method.

Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y).<br>
The most common technique is called Ordinary Least Squares:
given a regression line through the data we calculate the distance from each data point to the regression line, square it, and sum all of the squared errors together.

When there are a larger number of features or too many training instances to fit in memory, the use of different ways such as <b>Gradient Descent</b> is recommended.
